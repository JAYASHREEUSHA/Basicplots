{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAYASHREEUSHA/Basicplots/blob/main/Stemming_and_Lemmetization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming "
      ],
      "metadata": {
        "id": "BxR4kRBDxP3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "paragraph=\"Artificial Intelligence is focused on making computers that can think for themselves. This involves creating algorithms that enable computers to learn and make decisions on their own. Machine Learning, on the other hand, is a subset of AI that focuses on training computers to learn from data.\"\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "Stemmer=PorterStemmer()\n"
      ],
      "metadata": {
        "id": "ZtYKoeBTxRAo",
        "outputId": "3a47c1b4-b19e-4c86-835c-793c1655e5da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemming"
      ],
      "metadata": {
        "id": "YWxJDL45yg1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  words=[Stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "STOeEH09yiwB",
        "outputId": "cf121a7a-5955-46f0-d51d-5462f9e3f350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artifici intellig focus make comput think .', 'thi involv creat algorithm enabl comput learn make decis .', 'machin learn , hand , subset ai focus train comput learn data .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmetizn"
      ],
      "metadata": {
        "id": "CHJOmrMU45GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "paragraph=\"Artificial Intelligence is focused on making computers that can think for themselves. This involves creating algorithms that enable computers to learn and make decisions on their own. Machine Learning, on the other hand, is a subset of AI that focuses on training computers to learn from data.\"\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "Lemmatizer=WordNetLemmatizer()\n",
        "\n"
      ],
      "metadata": {
        "id": "UNWMi8zg5nl-",
        "outputId": "25d38b85-2875-49e6-bb4c-b2655b6fc29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  ad=nltk.word_tokenize(sentences[i])\n",
        "  ad=[Lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(words)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "mJ1pcHxA68i2",
        "outputId": "5324e201-39a7-47a6-dc74-d3634b12ee09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['machin learn , hand , subset ai focus train comput learn data .', 'machin learn , hand , subset ai focus train comput learn data .', 'machin learn , hand , subset ai focus train comput learn data .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0b2qQ1Bv9C7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}